# -*- coding: utf-8 -*-
"""internassign.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fGiAaab9B8uaVGzKmws1N4B7At8-GioT
"""


import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import math

df = pd.read_csv('C:/Users/91707/OneDrive - LNMIIT/cod/data_Sci/cars_price.csv')

train = df.copy()

train



"""getting the idea of the share of each feature of a column"""

df = df.drop('model',axis = 1)

train['make'].value_counts(normalize = True).plot.bar(figsize = (20,10),title = "make")
plt.show()
train['fuel_type'].value_counts(normalize = True).plot.bar(figsize = (20,10),title = "fuel_type")
plt.show()
train['transmission'].value_counts(normalize = True).plot.bar(figsize = (20,10),title = "transmission")
plt.show()
train['drive_unit'].value_counts(normalize = True).plot.bar(figsize = (20,10),title = "drive_unit")
plt.show()

"""from the above plots we can consider that

1.   weightage of brand in the data set means the number of cars the brand has sold
2.  petrol ,deisel, electric
1.   auto or mechanical
2.   type of drive
"""

train.groupby('mileage(kilometers)')['priceUSD'].median().plot()
plt.xlabel('mileage(kilometers)')
plt.ylabel('price')
plt.title("car Price vs mileage(kilometers)")
plt.show()


train.groupby('condition')['priceUSD'].median().plot()
plt.xlabel('condition')
plt.ylabel('price')
plt.title("car Price vs condition")
plt.show()


train.groupby('year')['priceUSD'].median().plot()
plt.xlabel('year')
plt.ylabel('price')
plt.title("car Price vs YearSold")
plt.show()


train.groupby('segment')['priceUSD'].median().plot()
plt.xlabel('segment')
plt.ylabel('price')
plt.title("car Price vs segment")
plt.show()

"""from the above plots we can clearly consider that 


1.   the price is high for the cars which have low mileage or which are travelled less kilometers
2.   the price decreased significantly in 1980s but afterwards ,it again rose
1.   the S segment mostly have a high price and segment A has lower.

**removing the nan valus**
"""

from statistics import mode
l = []
for i in df['segment']:
    if i != math.nan:
        l.append(i)
seg_mode = mode(l)

l = []
for i in df['volume(cm3)']:
    if i != math.nan:
        l.append(i)
vol_mode = (mode(l))

l = []

for i in df['drive_unit']:
    if i != math.nan:
        l.append(i)
drive_unit_mode =  mode(l)

for j in df.columns:
    print(j)
    ct = 0
    for i in range(len(df)) :
        if type(df[j][i]) != type('sdf'):
            if math.isnan(df[j][i]):
                if(j == 'volume(cm3)'):
                    df[j][i] = vol_mode
                if(j == 'drive_unit'):
                    df[j][i] = drive_unit_mode
                if(j == 'segment'):
                    df[j][i] = seg_mode
                #print('NaN')
                ct+=1
    print(ct)

train =df.copy()

"""**getting the dummie variables**"""

train = pd.get_dummies(train)

train

import math

"""spliting the test and train data"""

test = train.iloc[:math.ceil(len(train)*0.2),:]
train_  = train.iloc[math.ceil(len(train)*0.2):,:]

y_test = test.priceUSD
test.drop('priceUSD',axis = 1)

train_np = train_.to_numpy()
test_np = test.to_numpy()
y_train = train_.priceUSD
y_train = y_train.to_numpy()

y_test = y_test.to_numpy()

import xgboost
from sklearn.model_selection import RandomizedSearchCV
regressor = xgboost.XGBRegressor()

"""choosing the hyper parameters for the xgboost"""
'''
booster=['gbtree','gblinear']
base_score=[0.25,0.5,0.75,1]

n_estimators = [100, 500, 900, 1100, 1500]
max_depth = [2, 3, 5, 10, 15]
booster=['gbtree','gblinear']
learning_rate=[0.05,0.1,0.15,0.20]
min_child_weight=[1,2,3,4]

# Define the grid of hyperparameters to search
hyperparameter_grid = {
    'n_estimators': n_estimators,
    'max_depth':max_depth,
    'learning_rate':learning_rate,
    'min_child_weight':min_child_weight,
    'booster':booster,
    'base_score':base_score
    }

random_cv = RandomizedSearchCV(estimator=regressor,
            param_distributions=hyperparameter_grid,
            cv=5, n_iter=50,
            scoring = 'neg_mean_absolute_error',n_jobs = 4,
            verbose = 5, 
            return_train_score = True,
            random_state=42)
'''
len(y_train)
'''
random_cv.fit(train_np,y_train)

random_cv.best_estimator_
'''
regressor = xgboost.XGBRegressor(base_score=0.5, booster='gblinear', colsample_bylevel=None,
             colsample_bynode=None, colsample_bytree=None, gamma=None,
             gpu_id=-1, importance_type='gain', interaction_constraints=None,
             learning_rate=0.05, max_delta_step=None, max_depth=15,
             min_child_weight=2, missing=math.nan, monotone_constraints=None,
             n_estimators=1500, n_jobs=0, num_parallel_tree=None,
             random_state=0, reg_alpha=0, reg_lambda=0, scale_pos_weight=1,
             subsample=None, tree_method=None, validate_parameters=1,
             verbosity=None)

"""training with the best hyperparameters"""

regressor.fit(train_np,y_train)

import pickle

filename= 'finalized_model_xgbregressor.pkl'
pickle.dump(regressor,open(filename,'wb'))

y_pred = regressor.predict(test_np)

y_pred

"""MSE of 1.76 which is pretty good"""

from sklearn.metrics import mean_squared_error
print(len(train_np))
print(mean_squared_error(y_test,y_pred), ' = mean_s_error ')

test['predicted_price'] = y_pred

p =test.priceUSD
test.drop('priceUSD',axis=1)
test['original price'] = p

test1 = test[['original price','predicted_price']]

test1

"""exporting the csv file of predicted and original prices"""

test1.to_csv('C:/Users/91707/OneDrive - LNMIIT/cod/data_Sci/intern_assign_test.csv')