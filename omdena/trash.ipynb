{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SFCG-facebook data cleaning/preprocessing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBztumuctj0C"
      },
      "source": [
        "# Make a short cut from data into your drive then mount the drive \n",
        "import pandas as pd\n",
        "fb_data = pd.read_csv(\"/content/drive/MyDrive/facebook/cleaned_private_group_posts.csv\")\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZFOxuWgt_jE",
        "outputId": "d244e1a2-2393-4b6c-e938-d3c45c7a3e80"
      },
      "source": [
        "fb_data.columns"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['level_0', 'index', 'group_id', 'post_id', 'post_url', 'created_at',\n",
              "       'post_text', 'author_id', 'author_username', 'external_link', 'images',\n",
              "       'video_id', 'video', 'video_duration_seconds', 'video_thumbnail',\n",
              "       'video_watches', 'likes_count', 'comments_count', 'shares_count',\n",
              "       'reaction_count', 'reactions'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOH-GBFRu2H4"
      },
      "source": [
        "post_text=fb_data.post_text.dropna()"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0h1yNlRJmb4",
        "outputId": "eacacb86-7cce-4f1a-987b-e0bb9c43a023"
      },
      "source": [
        "import nltk \n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQ9PTagXJMVY"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "from string import punctuation\n",
        "from nltk.corpus import wordnet\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "sp = spacy.load('en_core_web_sm')\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOMXB9aTQjoL"
      },
      "source": [
        "# Text Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScPHGBrUvPbL"
      },
      "source": [
        "def text_norm(query):\n",
        "    '''\n",
        "    This function is for Text normalization\n",
        "      1. tokenization\n",
        "      2. Stemming\n",
        "      3. lemmatization\n",
        "    '''\n",
        "    lower_query = query.lower()\n",
        "    lower_tokens = lower_query.split(' ')\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in lower_tokens]\n",
        "    stemmed_tokens = [token for token in stemmed_tokens if token not in stopwords.words('english')]\n",
        "    # doc = sp(lower_query)\n",
        "    # lemm_tokens = [ token.lemma_ for token in doc]\n",
        "    # lemm_tokens = [token for token in lemm_tokens if token not in stopwords.words('english')]\n",
        "    \n",
        "    return \" \".join(stemmed_tokens)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkuRxNmrJ51P"
      },
      "source": [
        "def data_cleaning(sent):\n",
        "    \"\"\"\n",
        "    This function is meant to clean our data from the following:\n",
        "        1. Emails\n",
        "        2. Punctuations\n",
        "        3. Non-english words\n",
        "    \"\"\"\n",
        "    try:\n",
        "        sent = re.sub(\"\\S*@\\S*\\s?\", '', sent)\n",
        "        sent = sent.translate(table) \n",
        "        sent = \" \".join(w for w in nltk.wordpunct_tokenize(sent) if w.lower() in words or not w.isalpha())\n",
        "        sent = \"\".join([i for i in sent if not i.isdigit()])\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    return sent\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvOQKyp7JROT"
      },
      "source": [
        "for i in post_text[0:100]:\n",
        "  print(i)\n",
        "  print(text_norm(data_cleaning(i)))\n",
        "  print(\"========\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eugO1-V1TlOY"
      },
      "source": [
        "post_text = post_text.apply(lambda x: data_cleaning(x))\n",
        "post_text = post_text.apply(lambda x: text_norm(x))"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVYX_n63RkUw"
      },
      "source": [
        "post_text = pandas.DataFrame(post_text)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeFKxPhOWW2f",
        "outputId": "fd0717ba-cb61-4741-98f7-c80b1e993dc4"
      },
      "source": [
        "post_text"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       posit note thank prayer daughter lab result av...\n",
              "1       made acquaint realli good day yesterday today ...\n",
              "2       call 911 domest violenc paramed show took hosp...\n",
              "3       peopl truli understand domest violenc it' impo...\n",
              "4       prayer need got take girl crew get major blood...\n",
              "                              ...                        \n",
              "4121                         stop domest violenc men üôÜ‚Äç‚ôÇÔ∏è\n",
              "4122    hi everyone. thank add. want share thi free we...\n",
              "4124                             littl bit inspiration...\n",
              "4131    https://www.facebook.com/766848178/posts/10157...\n",
              "4132    https://www.facebook.com/766848178/posts/10157...\n",
              "Name: post_text, Length: 2255, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcgYzj_EQnZ_"
      },
      "source": [
        "# Feature extraction "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPoTQxIMQcHw"
      },
      "source": [
        "from sklearn import model_selection, preprocessing\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "\n",
        "import pandas, numpy, textblob, string"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUNRgKLcQ_mE"
      },
      "source": [
        "# create a count vectorizer object \n",
        "cv = CountVectorizer()\n",
        "cv.fit(post_text)\n",
        "\n",
        "train_cv =  cv.transform(post_text)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZpMUW06RL-7"
      },
      "source": [
        "cv.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjKb1gV0WApE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}